{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b514d1d-445a-444e-8349-0f4b31f272f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def date(x):\n",
    "    return datetime.datetime.strptime(x, '%Y-%m-%d')\n",
    "\n",
    "\n",
    "def train_test_split(df, train_split=0.8):\n",
    "    # This splits the data into train and test splits\n",
    "    obs = df.shape[0]\n",
    "    train_split_point = int(train_split * obs)\n",
    "    # Sort date columns to ensure its in order n then you can split\n",
    "    df['prediction_day'] = df['prediction_day'].apply(date)\n",
    "    df.sort_values(by='prediction_day', ascending=True)\n",
    "\n",
    "    df['time_int'] = df['prediction_day'].apply(lambda x: x.value)\n",
    "\n",
    "    # Split the data first and then do the rest\n",
    "    train = df.iloc[:train_split_point]\n",
    "    test = df.iloc[train_split_point:]\n",
    "\n",
    "    return train.values, test.values\n",
    "\n",
    "##Â Change this for RNN\n",
    "def train_validation_split(df, val_split=0.8):\n",
    "    import random\n",
    "    obs = df.shape[0]\n",
    "    train_split_point = int(val_split * obs)\n",
    "\n",
    "    random.shuffle(df)\n",
    "    # Split the data first and then do the rest\n",
    "    train = df[:train_split_point, :]\n",
    "    val = df[train_split_point:, :]\n",
    "\n",
    "    np.random.shuffle(train)\n",
    "    np.random.shuffle(val)\n",
    "\n",
    "    return train, val\n",
    "\n",
    "\n",
    "# make a function which is given train validation test and then does processing all to do with training data\n",
    "# regardless of the CV type, it does it correctly\n",
    "\n",
    "def sequencer(df, seq_len=5):\n",
    "    # This function is specifically for sequencing the data so it can be used with an RNN\n",
    "    sequential_data = []\n",
    "    prev_days = deque(maxlen=seq_len)\n",
    "\n",
    "    for i in df:  # iterate over the values\n",
    "        prev_days.append([n for n in i[1:-2]])\n",
    "        if len(prev_days) == seq_len:\n",
    "            sequential_data.append([np.array(prev_days), i[-2]])\n",
    "    X = []\n",
    "    y = []\n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def processing_cv(train, validation, test, seq=False, seq_length=5, fg = (False, 0)):\n",
    "    # This function uses the data from train, val test split and processes data in the correct manner so there is no\n",
    "    # data leakage. All scaling and sequencing is done so there is no data leakage to ensure the robustness of the\n",
    "    # model . Must sort again just in case\n",
    "    train_s = train[np.argsort(train[:, -1])]\n",
    "    validation_s = validation[np.argsort(validation[:, -1])]\n",
    "    test_s = test[np.argsort(test[:, -1])]\n",
    "\n",
    "    # Scale\n",
    "    train_scaled = train_s.copy()\n",
    "    validation_scaled = validation_s.copy()\n",
    "    test_scaled = test_s.copy()\n",
    "\n",
    "    if not fg[0]:\n",
    "\n",
    "        features_train = train_scaled[:, 1:-2]\n",
    "        features_validation = validation_scaled[:, 1:-2]\n",
    "        features_test = test_scaled[:, 1:-2]\n",
    "\n",
    "        scaler = StandardScaler().fit(features_train)\n",
    "\n",
    "        features_train1 = scaler.transform(features_train)\n",
    "        features_validation1 = scaler.transform(features_validation)\n",
    "        features_test1 = scaler.transform(features_test)\n",
    "\n",
    "        train_scaled[:, 1:-2] = features_train1\n",
    "        validation_scaled[:, 1:-2] = features_validation1\n",
    "        test_scaled[:, 1:-2] = features_test1\n",
    "\n",
    "    else:\n",
    "        col_num = fg[1]\n",
    "        features_train = train_scaled[:, 1:col_num]\n",
    "        features_validation = validation_scaled[:, 1:col_num]\n",
    "        features_test = test_scaled[:, 1:col_num]\n",
    "        scaler = StandardScaler().fit(features_train)\n",
    "        features_train1 = scaler.transform(features_train)\n",
    "        features_validation1 = scaler.transform(features_validation)\n",
    "        features_test1 = scaler.transform(features_test)\n",
    "        train_scaled[:, 1:col_num] = features_train1\n",
    "        validation_scaled[:, 1:col_num] = features_validation1\n",
    "        test_scaled[:, 1:col_num] = features_test1\n",
    "\n",
    "    if seq:\n",
    "        # sequence for rnn\n",
    "        x_train, y_train = sequencer(train_scaled, seq_len=seq_length)\n",
    "        x_val, y_val = sequencer(validation_scaled, seq_len=seq_length)\n",
    "        x_test, y_test = sequencer(test_scaled, seq_len=seq_length)\n",
    "\n",
    "        return x_train.astype(np.float), y_train.astype(np.float) \\\n",
    "            , x_val.astype(np.float), y_val.astype(np.float) \\\n",
    "            , x_test.astype(np.float), y_test.astype(np.float)\n",
    "\n",
    "    return train_scaled[:, 1:-2].astype(np.float), train_scaled[:, -2].astype(np.float), \\\n",
    "           validation_scaled[:, 1:-2].astype(np.float), validation_scaled[:, -2].astype(np.float), \\\n",
    "           test_scaled[:, 1:-2].astype(np.float), test_scaled[:, -2].astype(np.float)\n",
    "\n",
    "\n",
    "def processing_test(train, test, seq=False, seq_length=5):\n",
    "    # This function uses the data from train, val test split and processes data in the correct manner so there is no\n",
    "    # data leakage. All scaling and sequencing is done so there is no data leakage to ensure the robustness of the\n",
    "    # model . Must sort again just in case\n",
    "    train_s = train[np.argsort(train[:, -1])]\n",
    "    test_s = test[np.argsort(test[:, -1])]\n",
    "\n",
    "    # Scale\n",
    "    train_scaled = train_s.copy()\n",
    "    test_scaled = test_s.copy()\n",
    "\n",
    "    features_train = train_scaled[:, 1:-2]\n",
    "    features_test = test_scaled[:, 1:-2]\n",
    "\n",
    "    scaler = StandardScaler().fit(features_train)\n",
    "\n",
    "    features_train1 = scaler.transform(features_train)\n",
    "    features_test1 = scaler.transform(features_test)\n",
    "\n",
    "    train_scaled[:, 1:-2] = features_train1\n",
    "    test_scaled[:, 1:-2] = features_test1\n",
    "\n",
    "    if seq:\n",
    "        # sequence for rnn\n",
    "        x_train, y_train = sequencer(train_scaled, seq_len=seq_length)\n",
    "        x_test, y_test = sequencer(test_scaled, seq_len=seq_length)\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    return train_scaled[:, 1:-2].astype(np.float), train_scaled[:, -2].astype(np.float) \\\n",
    "        , test_scaled[:, 1:-2].astype(np.float), test_scaled[:, -2].astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "710ddfbe-7142-42cb-8763-390b8bc37946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('../../Data/Cleaned/Prior_FG_Present.csv')\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98291007-e5c2-4cbd-bf4b-2845774f7fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:200, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b2b548c5-2ddf-4ae7-9718-862fe050ce5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction_day</th>\n",
       "      <th>CNBC</th>\n",
       "      <th>IBDinvestors</th>\n",
       "      <th>MarketWatch</th>\n",
       "      <th>SJosephBurns</th>\n",
       "      <th>SPDJIndices</th>\n",
       "      <th>VPatelFX</th>\n",
       "      <th>benzinga</th>\n",
       "      <th>bespokeinvest</th>\n",
       "      <th>breakoutstocks</th>\n",
       "      <th>...</th>\n",
       "      <th>x352</th>\n",
       "      <th>x353</th>\n",
       "      <th>x354</th>\n",
       "      <th>x355</th>\n",
       "      <th>x356</th>\n",
       "      <th>x357</th>\n",
       "      <th>x358</th>\n",
       "      <th>x359</th>\n",
       "      <th>x360</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>1.762757</td>\n",
       "      <td>0.107444</td>\n",
       "      <td>-2.923540</td>\n",
       "      <td>-0.388096</td>\n",
       "      <td>1.332755</td>\n",
       "      <td>0.54277</td>\n",
       "      <td>-0.136163</td>\n",
       "      <td>0.812831</td>\n",
       "      <td>-0.087926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-06-15</td>\n",
       "      <td>2.392040</td>\n",
       "      <td>0.107444</td>\n",
       "      <td>-0.816843</td>\n",
       "      <td>-0.504146</td>\n",
       "      <td>0.443929</td>\n",
       "      <td>0.54277</td>\n",
       "      <td>-0.136163</td>\n",
       "      <td>2.989066</td>\n",
       "      <td>-0.087926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>-2.080261</td>\n",
       "      <td>-0.237079</td>\n",
       "      <td>0.387654</td>\n",
       "      <td>1.019944</td>\n",
       "      <td>0.923091</td>\n",
       "      <td>0.54277</td>\n",
       "      <td>-1.511802</td>\n",
       "      <td>2.989066</td>\n",
       "      <td>-0.087926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-06-19</td>\n",
       "      <td>-1.751510</td>\n",
       "      <td>0.925110</td>\n",
       "      <td>-5.348703</td>\n",
       "      <td>-0.049525</td>\n",
       "      <td>0.923091</td>\n",
       "      <td>0.54277</td>\n",
       "      <td>-1.511802</td>\n",
       "      <td>-0.665083</td>\n",
       "      <td>-0.087926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-06-20</td>\n",
       "      <td>2.663378</td>\n",
       "      <td>0.925110</td>\n",
       "      <td>1.621980</td>\n",
       "      <td>-0.049525</td>\n",
       "      <td>-0.059474</td>\n",
       "      <td>0.54277</td>\n",
       "      <td>-1.511802</td>\n",
       "      <td>1.379355</td>\n",
       "      <td>-0.087926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 381 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  prediction_day      CNBC  IBDinvestors  MarketWatch  SJosephBurns  \\\n",
       "0     2018-06-14  1.762757      0.107444    -2.923540     -0.388096   \n",
       "1     2018-06-15  2.392040      0.107444    -0.816843     -0.504146   \n",
       "2     2018-06-18 -2.080261     -0.237079     0.387654      1.019944   \n",
       "3     2018-06-19 -1.751510      0.925110    -5.348703     -0.049525   \n",
       "4     2018-06-20  2.663378      0.925110     1.621980     -0.049525   \n",
       "\n",
       "   SPDJIndices  VPatelFX  benzinga  bespokeinvest  breakoutstocks  ...  x352  \\\n",
       "0     1.332755   0.54277 -0.136163       0.812831       -0.087926  ...   0.0   \n",
       "1     0.443929   0.54277 -0.136163       2.989066       -0.087926  ...   0.0   \n",
       "2     0.923091   0.54277 -1.511802       2.989066       -0.087926  ...   0.0   \n",
       "3     0.923091   0.54277 -1.511802      -0.665083       -0.087926  ...   0.0   \n",
       "4    -0.059474   0.54277 -1.511802       1.379355       -0.087926  ...   0.0   \n",
       "\n",
       "   x353  x354  x355  x356  x357  x358  x359  x360    y  \n",
       "0   1.0   0.0   0.0   1.0   1.0   0.0   1.0   1.0  0.0  \n",
       "1   1.0   0.0   0.0   1.0   1.0   0.0   1.0   1.0  1.0  \n",
       "2   1.0   0.0   0.0   1.0   1.0   0.0   1.0   1.0  1.0  \n",
       "3   1.0   0.0   0.0   1.0   1.0   0.0   1.0   1.0  1.0  \n",
       "4   1.0   0.0   0.0   1.0   1.0   0.0   1.0   1.0  0.0  \n",
       "\n",
       "[5 rows x 381 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "25d9c4a0-8ecd-4805-a2bd-bde96b0b7e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9476c737-6884-46a5-8843-c57cb1cd819d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 382)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2435cc71-0ae3-48db-b9a1-d5a312a6dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, x3, x4, x5, x6 = processing_cv(train, test, test, seq=True, seq_length=3, fg = (False, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5b48fc11-05ca-44c1-a8e1-cde5eb64e123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 3, 379)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e5752096-176f-4e27-8ae4-c78b8c240bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None)\n"
     ]
    }
   ],
   "source": [
    "## Make sure theyre sorted \n",
    "train = train[np.argsort(train[:,-1])]\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits = 3)\n",
    "print(tscv)\n",
    "\n",
    "for train_index, test_index in tscv.split(train):\n",
    "    x_train, x_val = train[train_index], train[test_index]\n",
    "    \n",
    "    x_train, y_train, x_val, y_val, _1, _2 = processing_cv(x_train,x_val, test, seq = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0d5f7e23-eee5-4f00-b48e-0a4f1dd26674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 5, 379)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0fecd29a-56a9-4d1b-bfa6-8164b6926682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "349cb064-7555-4ebe-a374-1b5235b0bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(x_train.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bcb4bf9a-bea3-4cae-89c2-3a553103327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a1387a99-d42c-498f-8553-be8cc10d43c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 5s 317ms/step - loss: 0.8887 - accuracy: 0.5260 - val_loss: 0.6923 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.6358 - accuracy: 0.7424 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.5509 - accuracy: 0.7185 - val_loss: 0.6884 - val_accuracy: 0.5556\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4314 - accuracy: 0.7874 - val_loss: 0.6876 - val_accuracy: 0.5278\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4990 - accuracy: 0.7392 - val_loss: 0.6877 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.3547 - accuracy: 0.8224 - val_loss: 0.6867 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4224 - accuracy: 0.7911 - val_loss: 0.6861 - val_accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.3238 - accuracy: 0.8366 - val_loss: 0.6848 - val_accuracy: 0.5000\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.3250 - accuracy: 0.8528 - val_loss: 0.6841 - val_accuracy: 0.4722\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.3341 - accuracy: 0.7939 - val_loss: 0.6837 - val_accuracy: 0.4444\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.2686 - accuracy: 0.8639 - val_loss: 0.6842 - val_accuracy: 0.4444\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.2344 - accuracy: 0.8937 - val_loss: 0.6842 - val_accuracy: 0.4444\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.2579 - accuracy: 0.8927 - val_loss: 0.6843 - val_accuracy: 0.4722\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.2494 - accuracy: 0.8916 - val_loss: 0.6831 - val_accuracy: 0.5000\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.2324 - accuracy: 0.8910 - val_loss: 0.6825 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.1664 - accuracy: 0.9255 - val_loss: 0.6824 - val_accuracy: 0.5278\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.2054 - accuracy: 0.9170 - val_loss: 0.6828 - val_accuracy: 0.5833\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.1810 - accuracy: 0.9276 - val_loss: 0.6810 - val_accuracy: 0.5833\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.1818 - accuracy: 0.9065 - val_loss: 0.6791 - val_accuracy: 0.5833\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.2177 - accuracy: 0.8916 - val_loss: 0.6792 - val_accuracy: 0.5833\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.1295 - accuracy: 0.9352 - val_loss: 0.6797 - val_accuracy: 0.6111\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.1367 - accuracy: 0.9581 - val_loss: 0.6787 - val_accuracy: 0.5833\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.1543 - accuracy: 0.9457 - val_loss: 0.6744 - val_accuracy: 0.5556\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.1389 - accuracy: 0.9460 - val_loss: 0.6694 - val_accuracy: 0.5833\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.1145 - accuracy: 0.9682 - val_loss: 0.6682 - val_accuracy: 0.5556\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.1377 - accuracy: 0.9027 - val_loss: 0.6715 - val_accuracy: 0.5556\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.1201 - accuracy: 0.9634 - val_loss: 0.6765 - val_accuracy: 0.6389\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.1030 - accuracy: 0.9661 - val_loss: 0.6789 - val_accuracy: 0.6111\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0918 - accuracy: 0.9651 - val_loss: 0.6752 - val_accuracy: 0.6111\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.1374 - accuracy: 0.9418 - val_loss: 0.6651 - val_accuracy: 0.6389\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.1076 - accuracy: 0.9764 - val_loss: 0.6642 - val_accuracy: 0.6111\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.1817 - accuracy: 0.9439 - val_loss: 0.6697 - val_accuracy: 0.5833\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.0736 - accuracy: 0.9727 - val_loss: 0.6740 - val_accuracy: 0.5556\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.1107 - accuracy: 0.9595 - val_loss: 0.6704 - val_accuracy: 0.5556\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.1590 - accuracy: 0.9363 - val_loss: 0.6693 - val_accuracy: 0.6389\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0996 - accuracy: 0.9654 - val_loss: 0.6787 - val_accuracy: 0.6667\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0648 - accuracy: 0.9910 - val_loss: 0.6956 - val_accuracy: 0.6389\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0708 - accuracy: 0.9889 - val_loss: 0.7154 - val_accuracy: 0.5833\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.1246 - accuracy: 0.9702 - val_loss: 0.7222 - val_accuracy: 0.5833\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0891 - accuracy: 0.9637 - val_loss: 0.7430 - val_accuracy: 0.5833\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0835 - accuracy: 0.9879 - val_loss: 0.7675 - val_accuracy: 0.6389\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.0827 - accuracy: 0.9764 - val_loss: 0.7806 - val_accuracy: 0.5556\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.1022 - accuracy: 0.9615 - val_loss: 0.7810 - val_accuracy: 0.6111\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0700 - accuracy: 0.9709 - val_loss: 0.7921 - val_accuracy: 0.6111\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0738 - accuracy: 0.9702 - val_loss: 0.8123 - val_accuracy: 0.5833\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0597 - accuracy: 0.9889 - val_loss: 0.8256 - val_accuracy: 0.5556\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0766 - accuracy: 0.9730 - val_loss: 0.8638 - val_accuracy: 0.5556\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0499 - accuracy: 0.9851 - val_loss: 0.8875 - val_accuracy: 0.5833\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0469 - accuracy: 0.9910 - val_loss: 0.8885 - val_accuracy: 0.6111\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.0734 - accuracy: 0.9709 - val_loss: 0.8523 - val_accuracy: 0.6389\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_data=(x_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b49016-76ca-486e-bf2e-bb2c7976736f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
